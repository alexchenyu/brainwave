# ä»£ç ä¸­ LLM ä½¿ç”¨æƒ…å†µåˆ†æ

## ğŸ“Š æ¦‚è§ˆ

ä»£ç åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š
1. **è¯­éŸ³å¤„ç†** - ä½¿ç”¨ OpenAI Realtime APIï¼ˆä¿æŒä¸å˜ï¼‰
2. **æ–‡æœ¬å¤„ç†** - ä½¿ç”¨æ–‡æœ¬ LLM APIï¼ˆéœ€è¦æ›¿æ¢ä¸º GLMï¼‰

---

## ğŸ¤ è¯­éŸ³å¤„ç†éƒ¨åˆ†ï¼ˆä¿æŒä¸å˜ï¼‰

### æ–‡ä»¶ï¼š`openai_realtime_client.py`
**ç”¨é€”**ï¼šè¯­éŸ³è½¬æ–‡å­—çš„å®æ—¶å¤„ç†
**ä½¿ç”¨æ¨¡å‹**ï¼šOpenAI Realtime APIï¼ˆ`gpt-4o-realtime-preview`ï¼‰

```python
# openai_realtime_client.py:18
self.base_url = "wss://api.openai.com/v1/realtime"
```

**æµç¨‹**ï¼š
```
ç”¨æˆ·è¯­éŸ³ â†’ OpenAI Realtime WebSocket â†’ å®æ—¶è½¬å½•æ–‡å­—
```

âœ… **è¿™éƒ¨åˆ†ä¸éœ€è¦æ”¹åŠ¨**

---

## ğŸ“ æ–‡æœ¬å¤„ç†éƒ¨åˆ†ï¼ˆéœ€è¦æ›¿æ¢ä¸º GLMï¼‰

### 1. æ ¸å¿ƒ LLM å¤„ç†å™¨

#### æ–‡ä»¶ï¼š`llm_processor.py`

**å½“å‰å®ç°**ï¼š
- `GPTProcessor` - ä½¿ç”¨ OpenAI GPT æ¨¡å‹
- `GeminiProcessor` - ä½¿ç”¨ Google Gemini æ¨¡å‹

**ä½¿ç”¨çš„ OpenAI æ¨¡å‹**ï¼š
```python
# llm_processor.py:59
self.default_model = "gpt-4"
```

**API è°ƒç”¨æ–¹å¼**ï¼š
```python
# llm_processor.py:66-72 (å¼‚æ­¥æµå¼)
response = await self.async_client.chat.completions.create(
    model=model_name,
    messages=[{"role": "user", "content": all_prompt}],
    stream=True
)

# llm_processor.py:82-87 (åŒæ­¥)
response = self.sync_client.chat.completions.create(
    model=model_name,
    messages=[{"role": "user", "content": all_prompt}]
)
```

---

### 2. ä¸‰ä¸ª API ç«¯ç‚¹

#### æ–‡ä»¶ï¼š`realtime_server.py`

æ‰€æœ‰ç«¯ç‚¹éƒ½åœ¨è¿™ä¸ªæ–‡ä»¶ä¸­ï¼Œä½¿ç”¨ä¸åŒçš„æ¨¡å‹ï¼š

| ç«¯ç‚¹ | å½“å‰æ¨¡å‹ | è°ƒç”¨æ–¹å¼ | ä»£ç ä½ç½® | ç”¨é€” |
|------|---------|---------|---------|------|
| `/api/v1/readability` | **gpt-4o** | å¼‚æ­¥æµå¼ | è¡Œ 470, 474 | ä¼˜åŒ–æ–‡æœ¬å¯è¯»æ€§ |
| `/api/v1/ask_ai` | **o1-mini** | åŒæ­¥ | è¡Œ 500, 502 | å›ç­”é—®é¢˜ |
| `/api/v1/correctness` | **gpt-4o** | å¼‚æ­¥æµå¼ | è¡Œ 525, 529 | æ£€æŸ¥æ–‡æœ¬æ­£ç¡®æ€§ |

---

## ğŸ” è¯¦ç»†ä»£ç åˆ†æ

### ç«¯ç‚¹ 1: `/api/v1/readability` - æ–‡æœ¬å¯è¯»æ€§ä¼˜åŒ–

**ä½ç½®**ï¼š`realtime_server.py:455-481`

```python
@app.post("/api/v1/readability")
async def enhance_readability(request: ReadabilityRequest):
    prompt = PROMPTS.get('readability-enhance')

    # è·å– API Key
    api_key = request.api_key or OPENAI_API_KEY
    if not api_key:
        raise HTTPException(status_code=400, detail="OpenAI API key is required.")

    # åˆ›å»ºå¤„ç†å™¨ - ä½¿ç”¨ gpt-4o
    processor = get_llm_processor("gpt-4o", api_key=api_key)

    # å¼‚æ­¥æµå¼å¤„ç†
    async def text_generator():
        async for part in processor.process_text(request.text, prompt, model="gpt-4o"):
            yield part

    return StreamingResponse(text_generator(), media_type="text/plain")
```

**ç‰¹ç‚¹**ï¼š
- âœ… æµå¼è¾“å‡ºï¼ˆé€å­—è¿”å›ï¼‰
- âœ… å¼‚æ­¥å¤„ç†
- ä½¿ç”¨ OpenAI gpt-4o æ¨¡å‹

---

### ç«¯ç‚¹ 2: `/api/v1/ask_ai` - AI é—®ç­”

**ä½ç½®**ï¼š`realtime_server.py:483-506`

```python
@app.post("/api/v1/ask_ai")
def ask_ai(request: AskAIRequest):
    prompt = PROMPTS.get('ask-ai')

    # è·å– API Key
    api_key = request.api_key or OPENAI_API_KEY
    if not api_key:
        raise HTTPException(status_code=400, detail="OpenAI API key is required.")

    # åˆ›å»ºå¤„ç†å™¨ - ä½¿ç”¨ o1-mini
    processor = get_llm_processor("o1-mini", api_key=api_key)

    # åŒæ­¥å¤„ç†
    answer = processor.process_text_sync(request.text, prompt, model="o1-mini")

    return AskAIResponse(answer=answer)
```

**ç‰¹ç‚¹**ï¼š
- âŒ éæµå¼ï¼ˆç­‰å¾…å®Œæ•´å“åº”ï¼‰
- âŒ åŒæ­¥å¤„ç†
- ä½¿ç”¨ OpenAI o1-mini æ¨¡å‹

---

### ç«¯ç‚¹ 3: `/api/v1/correctness` - æ–‡æœ¬æ­£ç¡®æ€§æ£€æŸ¥

**ä½ç½®**ï¼š`realtime_server.py:508-536`

```python
@app.post("/api/v1/correctness")
async def check_correctness(request: CorrectnessRequest):
    prompt = PROMPTS.get('correctness-check')

    # è·å– API Key
    api_key = request.api_key or OPENAI_API_KEY
    if not api_key:
        raise HTTPException(status_code=400, detail="OpenAI API key is required.")

    # åˆ›å»ºå¤„ç†å™¨ - ä½¿ç”¨ gpt-4o
    processor = get_llm_processor("gpt-4o", api_key=api_key)

    # å¼‚æ­¥æµå¼å¤„ç†
    async def text_generator():
        async for part in processor.process_text(request.text, prompt, model="gpt-4o"):
            yield part

    return StreamingResponse(text_generator(), media_type="text/plain")
```

**ç‰¹ç‚¹**ï¼š
- âœ… æµå¼è¾“å‡º
- âœ… å¼‚æ­¥å¤„ç†
- ä½¿ç”¨ OpenAI gpt-4o æ¨¡å‹

---

## ğŸ”§ å…³é”®å‘ç°

### 1. ä½¿ç”¨çš„ OpenAI æ¨¡å‹

| æ¨¡å‹ | ä½¿ç”¨ä½ç½® | æ•°é‡ |
|------|---------|-----|
| **gpt-4o** | readability, correctness | 2 ä¸ªç«¯ç‚¹ |
| **o1-mini** | ask_ai | 1 ä¸ªç«¯ç‚¹ |
| gpt-4 | é»˜è®¤ï¼ˆæœªä½¿ç”¨ï¼‰ | 0 |

### 2. API è°ƒç”¨æ¨¡å¼

| æ¨¡å¼ | ç«¯ç‚¹ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|------|------|
| **å¼‚æ­¥æµå¼** | readability, correctness | å®æ—¶åé¦ˆï¼Œç”¨æˆ·ä½“éªŒå¥½ | å®ç°å¤æ‚ |
| **åŒæ­¥** | ask_ai | å®ç°ç®€å• | ç­‰å¾…æ—¶é—´é•¿ |

### 3. OpenAI Client ä½¿ç”¨æ–¹å¼

```python
# llm_processor.py:57-58
self.async_client = AsyncOpenAI(api_key=self.api_key)
self.sync_client = OpenAI(api_key=self.api_key)
```

**å¥½æ¶ˆæ¯**ï¼šOpenAI SDK æ”¯æŒ `base_url` å‚æ•°ï¼

```python
# å¯ä»¥è¿™æ ·æ”¹ä¸º GLM
AsyncOpenAI(api_key=api_key, base_url="http://your-glm-server:8000/v1")
OpenAI(api_key=api_key, base_url="http://your-glm-server:8000/v1")
```

---

## ğŸ¯ æ›¿æ¢ä¸º GLM çš„æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1ï¼šæœ€å°æ”¹åŠ¨ï¼ˆæ¨èï¼‰

åœ¨ `llm_processor.py` ä¸­ä¿®æ”¹ `GPTProcessor`ï¼Œæ·»åŠ  `base_url` å‚æ•°ï¼š

```python
class GPTProcessor(LLMProcessor):
    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.base_url = base_url or os.getenv("OPENAI_BASE_URL")  # æ–°å¢

        # ä½¿ç”¨è‡ªå®šä¹‰ base_url
        self.async_client = AsyncOpenAI(
            api_key=self.api_key,
            base_url=self.base_url  # æŒ‡å‘ä½ çš„ GLM æœåŠ¡
        )
        self.sync_client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
```

**ç¯å¢ƒå˜é‡é…ç½®**ï¼š
```bash
export OPENAI_BASE_URL="http://localhost:8000/v1"  # ä½ çš„ GLM æœåŠ¡åœ°å€
export OPENAI_API_KEY="dummy-key"  # GLM å¯èƒ½ä¸éœ€è¦çœŸå® key
```

**ä¼˜ç‚¹**ï¼š
- âœ… æ”¹åŠ¨æœ€å°
- âœ… å…¼å®¹ç°æœ‰ä»£ç 
- âœ… å¯ä»¥éšæ—¶åˆ‡æ¢å› OpenAI

---

### æ–¹æ¡ˆ 2ï¼šæ–°å¢ GLMProcessorï¼ˆæ›´æ¸…æ™°ï¼‰

åˆ›å»ºç‹¬ç«‹çš„ `GLMProcessor` ç±»ï¼š

```python
class GLMProcessor(LLMProcessor):
    def __init__(self, base_url: str = "http://localhost:8000/v1"):
        self.base_url = base_url
        self.api_key = "dummy-key"

        self.async_client = AsyncOpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        self.sync_client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        self.default_model = "glm-4"
```

**ä¿®æ”¹ `get_llm_processor`**ï¼š
```python
def get_llm_processor(model: str, api_key: Optional[str] = None) -> LLMProcessor:
    if model.startswith('glm'):
        return GLMProcessor(base_url=os.getenv("GLM_BASE_URL"))
    elif model.startswith(('gpt-', 'o1-')):
        return GPTProcessor(api_key=api_key)
    # ...
```

**ä¿®æ”¹ç«¯ç‚¹**ï¼š
```python
# realtime_server.py
processor = get_llm_processor("glm-4")  # æ”¹ä¸º glm-4
```

**ä¼˜ç‚¹**ï¼š
- âœ… ä»£ç ç»“æ„æ¸…æ™°
- âœ… å®¹æ˜“ç»´æŠ¤
- âœ… æ”¯æŒå¤šç§æ¨¡å‹

---

## ğŸ“‹ éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶æ¸…å•

### æ ¸å¿ƒæ–‡ä»¶ï¼ˆå¿…é¡»æ”¹ï¼‰

1. **`llm_processor.py`** - æ·»åŠ  GLM æ”¯æŒ
   - æ–°å¢ `GLMProcessor` ç±» æˆ–
   - ä¿®æ”¹ `GPTProcessor` æ·»åŠ  `base_url`

2. **`realtime_server.py`** - ä¿®æ”¹æ¨¡å‹è°ƒç”¨
   - è¡Œ 58: é»˜è®¤å¤„ç†å™¨
   - è¡Œ 470: readability ç«¯ç‚¹
   - è¡Œ 500: ask_ai ç«¯ç‚¹
   - è¡Œ 525: correctness ç«¯ç‚¹

### ä¸éœ€è¦æ”¹çš„æ–‡ä»¶

- âœ… `openai_realtime_client.py` - è¯­éŸ³å¤„ç†ï¼Œä¿æŒä¸å˜
- âœ… `prompts.py` - æç¤ºè¯ï¼Œä¸éœ€è¦æ”¹
- âœ… `static/main.js` - å‰ç«¯ä»£ç ï¼Œä¸éœ€è¦æ”¹

---

## ğŸš€ GLM éƒ¨ç½²è¦æ±‚

ä½ çš„ GLM-4.6-FP8 éœ€è¦æä¾› OpenAI å…¼å®¹çš„ APIï¼š

### å¿…é¡»æ”¯æŒçš„ç«¯ç‚¹

```
POST http://localhost:8000/v1/chat/completions
```

### è¯·æ±‚æ ¼å¼

```json
{
  "model": "glm-4",
  "messages": [
    {"role": "user", "content": "ä½ å¥½"}
  ],
  "stream": true,  // æµå¼å“åº”
  "temperature": 0.7,
  "max_tokens": 2048
}
```

### å“åº”æ ¼å¼ï¼ˆæµå¼ï¼‰

```json
data: {"choices":[{"delta":{"content":"ä½ "}}]}
data: {"choices":[{"delta":{"content":"å¥½"}}]}
data: [DONE]
```

---

## ğŸ’¡ æ¨èçš„æ›¿æ¢æ­¥éª¤

1. **ç¡®è®¤ GLM æœåŠ¡æ­£å¸¸è¿è¡Œ**
   - æµ‹è¯• API ç«¯ç‚¹
   - ç¡®è®¤å…¼å®¹ OpenAI æ ¼å¼

2. **ä¿®æ”¹ `llm_processor.py`**
   - æ·»åŠ  `GLMProcessor` ç±»
   - æ›´æ–° `get_llm_processor` å‡½æ•°

3. **ä¿®æ”¹ `realtime_server.py`**
   - å°† `"gpt-4o"` æ”¹ä¸º `"glm-4"`
   - å°† `"o1-mini"` æ”¹ä¸º `"glm-4"`

4. **é…ç½®ç¯å¢ƒå˜é‡**
   ```bash
   export GLM_BASE_URL="http://localhost:8000/v1"
   export USE_GLM="true"
   ```

5. **æµ‹è¯•ä¸‰ä¸ªç«¯ç‚¹**
   - `/api/v1/readability`
   - `/api/v1/ask_ai`
   - `/api/v1/correctness`

---

## ğŸ“Š æ€»ç»“

### è¯­éŸ³å¤„ç†ï¼ˆä¸æ”¹ï¼‰
- âœ… `openai_realtime_client.py` - OpenAI Realtime API
- âœ… WebSocket è¿æ¥ - `wss://api.openai.com/v1/realtime`

### æ–‡æœ¬å¤„ç†ï¼ˆéœ€è¦æ”¹ä¸º GLMï¼‰
- ğŸ“ `llm_processor.py` - æ·»åŠ  GLM æ”¯æŒ
- ğŸ“ `realtime_server.py` - 3 ä¸ªç«¯ç‚¹æ”¹ç”¨ GLM
  - `/api/v1/readability` (gpt-4o â†’ glm-4)
  - `/api/v1/ask_ai` (o1-mini â†’ glm-4)
  - `/api/v1/correctness` (gpt-4o â†’ glm-4)

### æ”¹åŠ¨é‡
- æ ¸å¿ƒä¿®æ”¹ï¼š2 ä¸ªæ–‡ä»¶
- ä»£ç è¡Œæ•°ï¼šçº¦ 50-100 è¡Œ
- å½±å“èŒƒå›´ï¼šä»…æ–‡æœ¬å¤„ç†ï¼Œè¯­éŸ³ä¸å—å½±å“

---

è¦æˆ‘å¼€å§‹ä¿®æ”¹ä»£ç å—ï¼Ÿè¿˜æ˜¯ä½ æƒ³å…ˆçœ‹çœ‹å…¶ä»–ä»€ä¹ˆä¿¡æ¯ï¼Ÿ
